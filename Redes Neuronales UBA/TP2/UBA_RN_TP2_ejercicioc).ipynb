{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"tanh\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"rect\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = W.dot(A_prev) + b\n",
    "    linear_cache = (A_prev, W, b)     \n",
    "    if activation == \"tanh\":\n",
    "        A = np.tanh(Z)\n",
    "    elif activation == \"rect\":\n",
    "        A = np.array(Z, copy=True)\n",
    "    activation_cache = Z\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = (AL - Y)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"rect\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"tanh\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache    \n",
    "    if activation == \"tanh\":\n",
    "        dZ = dA * (1 - np.power(np.tanh(activation_cache),2))\n",
    "    if activation == \"rect\":\n",
    "        dZ = dA * 1\n",
    "    A_prev, W, b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, x_val, y_val, layers_dims, learning_rate = 0.01, num_iterations = 1000):\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    costs_val = []\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        for j in range(0, (Y.shape[1])):\n",
    "            k = np.random.randint(Y.shape[1])\n",
    "            AL, caches =  L_model_forward(X[:, k].reshape(-1,1), parameters)\n",
    "            grads = L_model_backward(AL, Y[:, k].reshape(-1,1), caches)\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        AL, caches =  L_model_forward(X, parameters)\n",
    "        cost = (1/2) * np.sum(np.power(AL - Y, 2)) / (Y.shape[1])\n",
    "        cost = np.squeeze(cost) \n",
    "        costs.append(cost)\n",
    "        \n",
    "        AL_val, _ = L_model_forward(x_val, parameters)\n",
    "        cost_val = (1/2) * np.sum(np.power(AL_val - y_val, 2)) / (y_val.shape[1])\n",
    "        cost_val = np.squeeze(cost_val) \n",
    "        costs_val.append(cost_val)\n",
    "        if i % 250 == 0:\n",
    "            print (\"Iteración %i. Costo training: %f. Costo testing: %f\" %(i, cost, cost_val))\n",
    "        \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(np.squeeze(costs), 'blue', label = \"Error training\")\n",
    "    plt.plot(np.squeeze(costs_val), 'red', label = \"Error testing\")\n",
    "    plt.ylabel('Costo')\n",
    "    plt.xlabel('Iteración')\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = np.random.uniform(size = [500]) * 2 * np.pi\n",
    "y = np.random.uniform(size = [500]) * 2 * np.pi\n",
    "z = np.random.uniform(size = [500]) * 2 - 1\n",
    "entrada = np.array([x, y, z])\n",
    "f = (np.sin(x) + np.cos(y) + z).reshape(1,-1)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(entrada.T, f.T, test_size=0.3)\n",
    "train_x = train_x.T\n",
    "train_y = train_y.T\n",
    "test_x = test_x.T\n",
    "test_y = test_y.T\n",
    "\n",
    "print(entrada.shape)\n",
    "print(f.shape)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [train_x.shape[0], 16, 1]\n",
    "parameters = L_layer_model(train_x, train_y, test_x, test_y, layers_dims, learning_rate = 0.01, num_iterations = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, _ = L_model_forward(train_x, parameters)\n",
    "indices00 = np.argsort(train_x[0, :])\n",
    "indices01 = np.argsort(train_x[1, :])\n",
    "indices02 = np.argsort(train_x[2, :])\n",
    "\n",
    "pred_test, _ = L_model_forward(test_x, parameters)\n",
    "indices10 = np.argsort(test_x[0, :])\n",
    "indices11 = np.argsort(test_x[1, :])\n",
    "indices12 = np.argsort(test_x[2, :])\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(train_x[0, :][indices00], train_y[0][indices00], 'red')\n",
    "plt.plot(train_x[0, :], pred_train[0], '*')\n",
    "plt.title(\"Error de X en training\")\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(train_x[1, :][indices01], train_y[0][indices01], 'green')\n",
    "plt.plot(train_x[1, :], pred_train[0], '*')\n",
    "plt.title(\"Error de Y en training\")\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(train_x[2, :][indices02], train_y[0][indices02], 'blue')\n",
    "plt.plot(train_x[2, :], pred_train[0], '*')\n",
    "plt.title(\"Error de Z en training\")\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(test_x[0, :][indices10], test_y[0][indices10], 'red')\n",
    "plt.plot(test_x[0, :], pred_test[0], '*')\n",
    "plt.title(\"Error de X en testing\")\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(test_x[1, :][indices11], test_y[0][indices11], 'green')\n",
    "plt.plot(test_x[1, :], pred_test[0], '*')\n",
    "plt.title(\"Error de Y en testing\")\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(test_x[2, :][indices12], test_y[0][indices12], 'blue')\n",
    "plt.plot(test_x[2, :], pred_test[0], '*')\n",
    "plt.title(\"Error de Z en testing\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
